{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Take weight matrix parameters(input weights, recurrent weights and output weights) from 00\n",
    "Instantiate a trained network\n",
    "Retrain on Changing Amplitude (20-80), Changing Period (40-100), Clock-like Input. \n",
    "Transfer learning test.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd501689-2650-4b2c-96a6-865a3742b8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import snntorch as snn\n",
    "import matplotlib.pyplot as plt\n",
    "from snntorch import surrogate\n",
    "from snntorch import spikegen\n",
    "from snntorch import functional\n",
    "from snntorch import LIF\n",
    "from snntorch import spikeplot as splt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from warnings import warn\n",
    "import torch.nn as nn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7989c757-6655-4194-9dad-2d29f6c3eb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from classes.Sine_Wave_Dataset import SineWave1 #changing amp and per with resetting clock\n",
    "from classes.Custom_Loss import CustomLoss_task\n",
    "# from classes import RSNN3,train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72efeb04-a1ce-4350-b7e4-3f32d0c266f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import the trained recurrent matrix and output patrix\n",
    "data_dir = 'dataMP'\n",
    "file_name = 'level0_loss0_epoch495_batch39.npz'\n",
    "file_path = os.path.join(data_dir, file_name)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "        data = np.load(file_path)\n",
    "        #shape 200x200 and 1x200,epoch 495, batch 39, training data 24\n",
    "        l1_mx = torch.from_numpy(data['input_weights']).T  #2x200\n",
    "        rec_mx = torch.from_numpy(data['rec_weights']).T #200x200\n",
    "        l2_mx = torch.from_numpy(data['output_weights']).T #200x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd076534-d128-454c-9bc4-1afcb044a3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from classes.helper1 import conn_mx, hid_mx\n",
    "from classes.RLIF1 import RLIF1\n",
    "\n",
    "\n",
    "#set the rec_mx and output layer matrix to already trained model (trained on sinewave0 dataloader train_data_hpc0)\n",
    "#set the input layer 3x200: first row to first row, and third row to second row of the trained model\n",
    "class RSNN30(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RSNN30, self).__init__()\n",
    "        num_inputs = 3\n",
    "        num_hidden = 200\n",
    "        num_output = 1\n",
    "        beta = 0.85\n",
    "        pe_e = 0.16\n",
    "\n",
    "        # Define the dimensions\n",
    "        num_excitatory = 160\n",
    "        self.num_excitatory = num_excitatory\n",
    "        num_inhibitory = 40\n",
    "        self.false_neg = []\n",
    "        self.false_pos = []\n",
    "\n",
    "        #input to hidden layer\n",
    "        input_hid_mx = conn_mx(num_inputs, num_hidden, pe_e)\n",
    "        input_hid_mx[0, :] = l1_mx[0, :]  # First row of b is the first row of a\n",
    "        input_hid_mx[2, :] = l1_mx[1, :] \n",
    "        self.input_hid_mx = input_hid_mx\n",
    "        self.l1 = nn.Linear(num_inputs,num_hidden)\n",
    "        self.l1.weight.data = input_hid_mx.T\n",
    "\n",
    "        # Recurrent layer weight matrix        \n",
    "        self.rlif1 = RLIF1(reset_mechanism=\"zero\",threshold=1, beta=beta, linear_features=num_hidden, all_to_all=True)\n",
    "        self.rlif1.recurrent.weight.data = rec_mx.T\n",
    "\n",
    "        #hidden to output layer\n",
    "        # hid_out_mx = conn_mx(num_hidden,num_output,pe_e)\n",
    "        self.l2 = nn.Linear(num_hidden, num_output)\n",
    "        self.l2.weight.data = l2_mx.T\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        spk1,mem1 = self.rlif1.init_rleaky()\n",
    "        self.spk1_rec = []\n",
    "        self.cur2_rec = []\n",
    "\n",
    "        # print(inputs.shape)\n",
    "        for step in range(inputs.shape[0]): #300\n",
    "            cur_input = inputs[step,:]\n",
    "            cur1 = self.l1(cur_input)\n",
    "            spk1,mem1 = self.rlif1(cur1, spk1, mem1)\n",
    "            cur2 = self.l2(spk1)\n",
    "\n",
    "            self.spk1_rec.append(spk1)\n",
    "            self.cur2_rec.append(cur2)\n",
    "\n",
    "        self.spk1_rec = torch.stack(self.spk1_rec)\n",
    "        self.cur2_rec = torch.stack(self.cur2_rec)\n",
    "        cur2_rec = self.cur2_rec\n",
    "\n",
    "        \n",
    "        \n",
    "        return self.cur2_rec, self.spk1_rec\n",
    "\n",
    "    def positive_negative_weights(self):\n",
    "\n",
    "        excitatory_weights = self.rlif1.recurrent.weight.data[:, :self.num_excitatory]\n",
    "        inhibitory_weights = self.rlif1.recurrent.weight.data[:, self.num_excitatory:]\n",
    "\n",
    "        #save the number of positives in inhibitory and negatives in excitatory region\n",
    "        num_false_neg = torch.sum(excitatory_weights < 0).item()\n",
    "        num_false_pos = torch.sum(inhibitory_weights > 0).item()\n",
    "\n",
    "        self.false_neg.append(num_false_neg)\n",
    "        self.false_pos.append(num_false_pos)\n",
    "\n",
    "        # Clamp switched sign values at 0\n",
    "        excitatory_weights.clamp_(min=0)\n",
    "        inhibitory_weights.clamp_(max=0)\n",
    "\n",
    "        mu = -0.64\n",
    "        sigma = 0.51\n",
    "\n",
    "\n",
    "        #change the code so that for any vanishing excitatory neuron, populate another excitatory.\n",
    "\n",
    "        #following code picks random indices from excitatory and inhibitory originating weights\n",
    "        #for the number of num_false_neg and num_false_neg for inhibitory and excitatory originating weights respectively\n",
    "        #assigns them with the lognormal dist\n",
    "        excitatory_zero_indices = (self.rlif1.recurrent.weight.data[:, :self.num_excitatory] == 0).nonzero(as_tuple=True)\n",
    "        inhibitory_zero_indices = (self.rlif1.recurrent.weight.data[:, self.num_excitatory:] == 0).nonzero(as_tuple=True)\n",
    "\n",
    "        if (len(excitatory_zero_indices) > num_false_pos):\n",
    "            excitatory_sampled_indices = torch.stack([\n",
    "                    excitatory_zero_indices[0][torch.randint(len(excitatory_zero_indices[0]), (num_false_pos,))],\n",
    "                    excitatory_zero_indices[1][torch.randint(len(excitatory_zero_indices[1]), (num_false_pos,))]\n",
    "                ], dim=1)\n",
    "\n",
    "            # generating self.excitatory_changes number of lognormal values\n",
    "            new_excitatory_values = torch.from_numpy(np.random.lognormal(mean=mu, sigma=sigma, size=num_false_pos)).float()\n",
    "            self.rlif1.recurrent.weight.data[excitatory_sampled_indices[:, 0], excitatory_sampled_indices[:, 1]] = new_excitatory_values\n",
    "\n",
    "        if (len(inhibitory_zero_indices) > num_false_neg):\n",
    "            inhibitory_sampled_indices = torch.stack([\n",
    "                    inhibitory_zero_indices[0][torch.randint(len(inhibitory_zero_indices[0]), (num_false_neg,))],\n",
    "                    inhibitory_zero_indices[1][torch.randint(len(inhibitory_zero_indices[1]), (num_false_neg,))]\n",
    "                ], dim=1)\n",
    "\n",
    "            new_inhibitory_values = -torch.from_numpy(np.random.lognormal(mean=mu, sigma=sigma, size=num_false_neg)).float()\n",
    "            self.rlif1.recurrent.weight.data[inhibitory_sampled_indices[:, 0], self.num_excitatory + inhibitory_sampled_indices[:, 1]] = new_inhibitory_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a16e2e0-22ef-4578-96cc-242571053965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from classes.helper1 import count_spikes\n",
    "\n",
    "def train_modelA(args):\n",
    "    model, optimizer, dataloader,step_dataloader, criterion, criterium_idx, num_epochs, num_timesteps= args\n",
    "    model.train()\n",
    "    weights = model.rlif1.recurrent.weight.data\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs, targets\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = torch.empty(0, dtype=torch.float32, requires_grad=True)\n",
    "            firing_rate_per_batch = torch.empty(0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "            for input, target in zip(inputs, targets):\n",
    "                output, spikes = model(input)\n",
    "                spikes = spikes.T\n",
    "                outputs = torch.cat((outputs, output.view(1, -1)))\n",
    "                firing_rate = count_spikes(spikes)\n",
    "                firing_rate_per_batch = torch.cat((firing_rate_per_batch, firing_rate))\n",
    "                \n",
    "            loss = criterion(outputs, targets, firing_rate_per_batch)\n",
    "            print(\"loss: \", loss)\n",
    "            \n",
    "            loss.backward()\n",
    "            # print(model.l2.weight.grad)\n",
    "            zero_idxs = torch.where(weights == 0, 1, 0)  # Create a matrix to identify where zeroes are initialized in weight matrix\n",
    "            optimizer.step()\n",
    "\n",
    "            #maintain initial sparsity - #keep previously 0 weights 0\n",
    "            weights[zero_idxs==True] = 0\n",
    "            model.positive_negative_weights()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                np.savez(f'dataMP/level{7}_loss{criterium_idx}_epoch{epoch}_batch{i}.npz',\n",
    "                         task_loss=criterion.task_loss.item(),\n",
    "                         firing_rate_loss = criterion.rate_loss.item(),\n",
    "                         spikes=spikes.detach().numpy(),\n",
    "                         input_weights=model.l1.weight.data.detach().numpy(),\n",
    "                         rec_weights=model.rlif1.recurrent.weight.data.detach().numpy(),\n",
    "                         output_weights=model.l2.weight.data.detach().numpy(),\n",
    "                         inputs=inputs.detach().numpy(),\n",
    "                         outputs=outputs.detach().numpy(),\n",
    "                         targets=targets.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df1e422-a869-40cd-a02d-f6a8cfb10af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(5831.5884, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(3954.1270, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(2630.1145, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(2579.0317, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(2016.9366, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1766.3584, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1498.3480, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1379.9786, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1241.7091, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1361.6896, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1067.6938, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1341.5569, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1524.8998, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(859.3832, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1486.7693, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1143.5688, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1185.8330, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(967.4343, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1092.4482, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(835.9145, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(862.8719, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1097.7382, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(842.4641, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(967.5689, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(751.8902, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(1181.4155, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(759.4691, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(597.6139, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(627.2707, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(569.8939, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(616.6779, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(663.8171, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(779.8282, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(790.1910, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(778.0458, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(760.7386, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(656.1368, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(676.6234, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(824.8882, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(815.8901, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(607.2137, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(627.9661, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(758.8463, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(686.6313, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(685.2145, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(564.3073, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(657.4239, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(606.9434, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(812.8906, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(464.5768, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(647.0759, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(810.2017, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(744.5635, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(667.5732, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(616.3593, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(610.4896, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(926.6951, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(660.5056, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(552.5956, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(650.0621, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(630.4302, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(827.5526, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(541.5466, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(594.2004, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(706.3744, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(627.2305, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(580.7509, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(603.1939, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(722.6963, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(711.2745, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(718.5822, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(646.7961, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(399.0641, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(510.9580, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(547.6758, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(674.8704, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(753.1650, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(767.0151, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(636.8576, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(729.3953, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(682.4491, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(634.6838, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(737.6916, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(686.1396, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(669.0945, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(615.1089, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(564.6100, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(671.7518, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(444.1839, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(575.3441, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(602.3185, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(539.9211, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(645.7206, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(521.0578, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(586.4699, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(679.4309, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(591.7401, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(642.5861, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(809.8175, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(488.8336, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(582.6498, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(791.9903, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(656.1707, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(601.7930, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(643.8587, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(575.2922, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(804.8649, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(728.7416, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(491.4783, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(669.3420, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(512.6589, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(572.3467, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(706.3434, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(615.4468, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(772.8447, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(688.3547, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(577.2799, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(479.6217, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(731.8345, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(660.7115, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(664.4276, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(574.5059, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(716.1631, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(606.8887, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#uses the SineWaveClass 1\n",
    "dataset7 = SineWave1.SineWaveDataset1('train_data/train_data_sine_hpc.csv')\n",
    "dataloader7 = DataLoader(dataset7, batch_size=25, shuffle=True)\n",
    "loss_task = CustomLoss_task.CustomLoss_task()\n",
    "net_70 = RSNN30()\n",
    "optimizer_70 = torch.optim.Adam(net_70.parameters(),lr=0.03)\n",
    "num_epochs = 1000\n",
    "num_timesteps = 300\n",
    "\n",
    "train_modelA([net_70, optimizer_70,dataloader7,7, loss_task, 0, 500, num_timesteps])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d960d6-2551-4bd1-9fc9-e3f25e290d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
